% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/stats.R
\name{em}
\alias{em}
\title{EM algorithm}
\usage{
em(data, params, stepE, stepM, loglik, thresh.cvg = 10^(-3),
  nb.iters = 10^3, verbose = 1)
}
\arguments{
\item{data}{data}

\item{params}{list with the initial values of the parameters}

\item{stepE}{function implementing the E step taking \code{data} and \code{params} as inputs; can be parallelized}

\item{stepM}{function implementing the M step taking \code{data}, \code{params} and the output of \code{stepE} as inputs; can be parallelized}

\item{loglik}{function taking \code{data} and \code{params} as inputs, and returning the value of the observed-data log-likelihood; can be parallelized}

\item{thresh.cvg}{threshold on the absolute difference between the observed-data log-likelihood of two successive iterations below which convergence is reached}

\item{nb.iters}{number of iterations}

\item{verbose}{verbosity level (0/1/2)}
}
\value{
list with MLEs of the parameters and values of the observed-data log-likelihood
}
\description{
Run the EM algorithm.
}
\examples{
\dontrun{## I. example of the EM algorithm for univariate Gaussian mixture

## I.1. simulate some data
simulDat <- function(K=2, N=100, gap=6){
  means <- seq(0, gap*(K-1), gap)
  stdevs <- runif(n=K, min=0.5, max=1.5)
  tmp <- floor(rnorm(n=K-1, mean=floor(N/K), sd=5))
  ns <- c(tmp, N - sum(tmp))
  memberships <- as.factor(matrix(unlist(lapply(1:K, function(k){rep(k, ns[k])})),
                           ncol=1))
  data <- matrix(unlist(lapply(1:K, function(k){
    rnorm(n=ns[k], mean=means[k], sd=stdevs[k])
  })))
  new.order <- sample(1:N, N)
  data <- data[new.order]
  rownames(data) <- NULL
  memberships <- memberships[new.order]
  return(list(data=data, memberships=memberships,
              means=means, stdevs=stdevs, weights=ns/N))
}
set.seed(1859)
K <- 3
N <- 300
simul <- simulDat(K, N)
simul$means
simul$stdevs
simul$weights

## I.2. visualize the data
hist(simul$data, breaks=30, freq=FALSE, col="grey", border="white",
     main="Simulated data from univariate Gaussian mixture",
     ylab="", xlab="data", las=1,
     xlim=c(-4,15), ylim=c(0,0.28))

## I.3. define functions required to run the EM algorithm

loglik <- function(data, params){
  sum(sapply(data, function(datum){
    log(sum(unlist(Map(function(mu, sigma, weight){
      weight * dnorm(x=datum, mean=mu, sd=sigma)
    }, params$means, params$stdevs, params$weights))))
  }))
}
loglik(simul$data, simul[-c(1,2)])

## function performing the E step
stepE <- function(data, params){
  N <- length(data)
  K <- length(params$means)
  tmp <- matrix(unlist(lapply(data, function(datum){
    norm.const <- sum(unlist(Map(function(mu, sigma, weight){
      weight * dnorm(x=datum, mean=mu, sd=sigma)
    }, params$means, params$stdevs, params$weights)))
    unlist(Map(function(mu, sigma, weight){
        weight * dnorm(x=datum, mean=mu, sd=sigma) / norm.const
      }, params$means[-K], params$stdevs[-K], params$weights[-K]))
  })), ncol=K-1, byrow=TRUE)
  membership.probas <- cbind(tmp, apply(tmp, 1, function(x){1 - sum(x)}))
  names(membership.probas) <- NULL
  return(membership.probas)
}
head(mb.pr <- stepE(simul$data, simul[-c(1,2)]))

stepM <- function(data, params, out.stepE){
  N <- length(data)
  K <- length(params$means)
  sum.membership.probas <- apply(out.stepE, 2, sum)
  ## MLEs of the means
  new.means <- sapply(1:K, function(k){
    sum(unlist(Map("*", out.stepE[,k], data))) /
    sum.membership.probas[k]
  })
  ## MLEs of the standard deviations
  new.stdevs <- sapply(1:K, function(k){
      sqrt(sum(unlist(Map(function(p.ki, x.i){
      p.ki * (x.i - new.means[k])^2
    }, out.stepE[,k], data))) /
    sum.membership.probas[k])
  })
  ## MLEs of the weights
  new.weights <- sapply(1:K, function(k){
    1/N * sum.membership.probas[k]
  })
  return(list(means=new.means, stdevs=new.stdevs, weights=new.weights))
}
stepM(simul$data, simul[-c(1,2)], mb.pr)

## I.4. run the EM algorithm
params0 <- list(means=runif(n=K, min=min(simul$data), max=max(simul$data)),
                stdevs=rep(1, K),
                weights=rep(1/K, K))
fit <- em(data=simul$data, params=params0,
          stepE=stepE, stepM=stepM, loglik=loglik,
          verbose=1)

## I.5. plot the log likelihood per iteration
plot(fit$logliks, xlab="iterations", ylab="log-likelihood",
     main="Convergence of the EM algorithm", type="b")

## I.6. plot the data along with the inferred density
hist(simul$data, breaks=30, freq=FALSE, col="grey", border="white",
     main="Simulated data from univariate Gaussian mixture",
     ylab="", xlab="data", las=1,
     xlim=c(-4,15), ylim=c(0,0.28))
rx <- seq(from=min(simul$data), to=max(simul$data), by=0.1)
ds <- lapply(1:K, function(k){dnorm(x=rx, mean=fit$params$means[k], sd=fit$params$stdevs[k])})
f <- sapply(1:length(rx), function(i){
  fit$params$weights[1] * ds[[1]][i] + fit$params$weights[2] * ds[[2]][i] + fit$params$weights[3] * ds[[3]][i]
})
lines(rx, f, col="red", lwd=2)

## I.7. look at the classification of the data
mb.pr <- stepE(simul$data, fit$params)
memberships <- apply(mb.pr, 1, function(x){which(x > 0.8)})
table(memberships)

## II. example of the EM algorithm for univariate linear mixed model

## II.1. simulate some data
## TODO

## II.2. visualize the data
## TODO

## II.3. define functions required to run the EM algorithm
## TODO

loglik <- function(data, params){
}

stepE <- function(data, params){
}

stepM <- function(data, params, out.stepE){
}

## II.4. run the EM algorithm
params0 <- list()
fit <- em(data=simul$data, params=params0,
          stepE=stepE, stepM=stepM, loglik=loglik,
          verbose=1)

## II.5. plot the log likelihood per iteration
plot(fit$logliks, xlab="iterations", ylab="log-likelihood",
     main="Convergence of the EM algorithm", type="b")

## III. other possible examples of the EM algorithm
## * factor analysis
## * hidden Markov models
## * multivariate Student distribution
## * robust regression models
## * censored/truncated models
## * ...
}
}
\author{
Timothee Flutre
}
